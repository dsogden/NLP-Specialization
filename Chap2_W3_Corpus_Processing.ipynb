{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoagt0CsyWo3WKokC+Yf0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsogden/NLP-Specialization/blob/main/Chap2_W3_Corpus_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyfGecl4G8Sk",
        "outputId": "57edf963-1991-49ae-ac29-910b1c1ca808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laK3huFqHEDD",
        "outputId": "656eb468-ee3e-4989-9e21-2b09570a363c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special characters\n",
        "corpus_cleaned = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
        "corpus_cleaned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x-zlbTdAKu4X",
        "outputId": "50e2e6b4-f273-45bf-a0a7-d69c267ed506"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'learning makes me happy. i am happy because i am learning! '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text splitting"
      ],
      "metadata": {
        "id": "9OoW1j8vLM7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_date = 'Sat May  9 07:33:35 CEST 2020'\n",
        "date_parts = input_date.split(' ')\n",
        "print(f'date parts: {date_parts}')\n",
        "\n",
        "time_parts = date_parts[4].split(':')\n",
        "print(f'time parts: {time_parts}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k11lzTJfK_QW",
        "outputId": "02163242-16f7-4acb-9e4e-32a1e91ef120"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date parts: ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
            "time parts: ['07', '33', '35']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence tokenizing"
      ],
      "metadata": {
        "id": "MXgGygBkLk6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i am happy because i am learning.'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f'{sentence} --> {tokenized_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tJU1mXJLcC0",
        "outputId": "18beacb5-c4a6-442d-a0b8-e519f6a927df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning. --> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine the length of each word in the tokenized sentence\n",
        "word_lengths = [(word, len(word)) for word in tokenized_sentence]\n",
        "print(f'lengths of words: {word_lengths}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_eSdEN_L0g7",
        "outputId": "196fe1b2-b452-4ab3-bf2c-bbda3378525b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lengths of words: [('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Grams"
      ],
      "metadata": {
        "id": "UW776bNYMbRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_trigram(tokenized_sentence):\n",
        "    '''\n",
        "    Prints all trigrams in a given tokenized sentence\n",
        "\n",
        "    Args:\n",
        "        tokenized sentence: The words list.\n",
        "\n",
        "    Returns:\n",
        "        No output\n",
        "    '''\n",
        "\n",
        "    n = len(tokenized_sentence)\n",
        "    offset = 3\n",
        "    for i in range(n - offset + 1):\n",
        "        trigram = tokenized_sentence[i: i + offset]\n",
        "        print(trigram)\n",
        "\n",
        "sentence_to_trigram(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFeEqiwIMUFe",
        "outputId": "04097648-679b-4b25-aa65-b383883d5774"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourgram = ['i', 'am', 'happy', 'because']\n",
        "trigram = fourgram[0: -1]\n",
        "print(trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz3SmcGlM6wM",
        "outputId": "589a8bb2-c677-42c2-ad34-ee61905a6890"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start and end of sentence word <s> and <e>"
      ],
      "metadata": {
        "id": "dti-jqNpOnTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "tokenized_sentence = ['<s>'] * (n - 1) + tokenized_sentence + ['<e>']\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9heGckk2Oluh",
        "outputId": "884e6ed8-f1ba-4cfb-caee-8586b2cd9c7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '<e>']\n"
          ]
        }
      ]
    }
  ]
}