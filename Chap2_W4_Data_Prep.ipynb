{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pp8q-tjmG2o0J6GeE6hkgse-z5JkUoLm",
      "authorship_tag": "ABX9TyPmi4dAW5JJ7DdlUsI85oyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsogden/NLP-Specialization/blob/main/Chap2_W4_Data_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_Ym2BJmm-0",
        "outputId": "05e171e3-547a-4043-e905-80e90a1f082d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji==1.4.1 in /usr/local/lib/python3.10/dist-packages (1.4.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "! pip install emoji==1.4.1 --upgrade\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import emoji\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def get_dict(data):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        K: the number of negative samples\n",
        "        data: the data you want to pull from\n",
        "        indices: a list of word indices\n",
        "    Output:\n",
        "        word_dict: a dictionary with the weighted probabilities of each word\n",
        "        word2Ind: returns dictionary mapping the word to its index\n",
        "        Ind2Word: returns dictionary mapping the index to its word\n",
        "    \"\"\"\n",
        "    #\n",
        "    #     words = nltk.word_tokenize(data)\n",
        "    words = sorted(list(set(data)))\n",
        "    n = len(words)\n",
        "    idx = 0\n",
        "    # return these correctly\n",
        "    word2Ind = {}\n",
        "    Ind2word = {}\n",
        "    for k in words:\n",
        "        word2Ind[k] = idx\n",
        "        Ind2word[idx] = k\n",
        "        idx += 1\n",
        "    return word2Ind, Ind2word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "ylLbptKGnUcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n",
        "print(f'Corpus: {corpus}')\n",
        "\n",
        "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "print(f'data after cleaning: {data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vUVQdywnLgT",
        "outputId": "1cc12e81-3948-41b8-e2d0-644b1ad4ae03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
            "data after cleaning: Who ❤️ \"word embeddings\" in 2020. I do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize cleaned data\n",
        "data = word_tokenize(data)\n",
        "print(f'after tokenization: {data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMfh7e4Vnm_0",
        "outputId": "df0cd913-bff6-4981-e3a3-46e320c05098"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after tokenization: ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Initial list of data: {data}')\n",
        "\n",
        "data = [\n",
        "    char.lower() for char in data\n",
        "    if char.isalpha()\n",
        "    or char == '.'\n",
        "    or emoji.get_emoji_regexp().search(char)\n",
        "]\n",
        "\n",
        "print(f'After cleaning: {data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx50jH5an7qm",
        "outputId": "d34e943a-60d1-4ccc-8a3a-ad103ab0835d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial list of data: ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n",
            "After cleaning: ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "    data = word_tokenize(data)\n",
        "    data = [\n",
        "        char.lower() for char in data\n",
        "        if char.isalpha()\n",
        "        or char == '.'\n",
        "        or emoji.get_emoji_regexp().search(char)\n",
        "    ]\n",
        "    return data"
      ],
      "metadata": {
        "id": "UP3ZSIPjoaYA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'I am happy because I am learning'\n",
        "print(f'Corpus: {corpus}')\n",
        "words = tokenize(corpus)\n",
        "print(f'Words (tokens): {words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPqNOQmMpqm4",
        "outputId": "688a72b9-5409-4803-96cf-bc3c1ddba173"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: I am happy because I am learning\n",
            "Words (tokens): ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sliding window of words"
      ],
      "metadata": {
        "id": "flo7gVH-qEXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C): i] + words[(i + 1): (i + C + 1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "UYHRWRtqpz1b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in get_windows(words, 2):\n",
        "    print(f'x = {x} \\t y = {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ykM-Rotqh4Y",
        "outputId": "eaaf552f-306b-40ee-ecc3-3bceac2a784b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = ['i', 'am', 'because', 'i'] \t y = happy\n",
            "x = ['am', 'happy', 'i', 'am'] \t y = because\n",
            "x = ['happy', 'because', 'am', 'learning'] \t y = i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming words into vectors"
      ],
      "metadata": {
        "id": "SKaHpTUesK1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_2_ind, ind_2_word = get_dict(words)\n",
        "word_2_ind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ZEX-Prr4tN",
        "outputId": "c42908f4-3656-4fb3-a355-48f4f7b6d946"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting one-hot word vectors"
      ],
      "metadata": {
        "id": "EvBk0Zh3s1o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = len(word_2_ind)\n",
        "center_word_vector = np.zeros(V)\n",
        "center_word_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9sWn5UAshhC",
        "outputId": "030ddd44-a378-47f0-cdb7-bfca86825e92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = word_2_ind['happy']\n",
        "center_word_vector[n] = 1\n",
        "center_word_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAQOP9rWtBLI",
        "outputId": "611e4f3e-a427-4956-af50-161026345ec6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(word, word_2_ind, V):\n",
        "    one_hot_vector = np.zeros(V)\n",
        "    one_hot_vector[word_2_ind[word]] = 1\n",
        "    return one_hot_vector"
      ],
      "metadata": {
        "id": "RweveGDGtZ27"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder('happy', word_2_ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y0KDw4At0kI",
        "outputId": "b64b56ae-6e1c-42a6-a266-42e9724aa0d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting context words"
      ],
      "metadata": {
        "id": "CYtub5-tuRuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_words = ['i', 'am', 'because', 'i']\n",
        "context_word_vectors = [\n",
        "    one_hot_encoder(word, word_2_ind, V) for word in context_words\n",
        "]"
      ],
      "metadata": {
        "id": "8l_ssT4st50q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Context word vectors: {context_word_vectors}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFAjsZNVudGq",
        "outputId": "5c3c7394-8b52-415e-cea2-63ce0a48efb8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context word vectors: [array([0., 0., 0., 1., 0.]), array([1., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0.]), array([0., 0., 0., 1., 0.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(context_word_vectors, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg7HHceiug71",
        "outputId": "06e44cf1-26ea-4e54-d435-f790d5015f43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}